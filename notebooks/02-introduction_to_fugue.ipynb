{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Fugue\n",
    "\n",
    "The [Fugue](https://github.com/fugue-project/fugue/) project aims to make big data effortless by accelerating iteration speed and providing a simpler interface for users to utilize distributed computing engines.\n",
    "\n",
    "Here we just take a quick look at Fugue. These examples are taken from the [Fugue in 10 minutes](https://fugue-tutorials.readthedocs.io/tutorials/quick_look/ten_minutes.html#partitioning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The simplest way to scale pandas based code to Spark or Dask is with the transform() function. With the addition of this minimal wrapper, we can bring existing Pandas and Python code to distributed execution with minimal refactoring. The transform() function also provides quality of life enhancements that can eliminate boilerplate code for users.\n",
    "\n",
    "Letâ€™s quickly demonstrate how this concept can be applied. In the following code snippets below we will train a model using scikit-learn and pandas. Then we will perform predictions using this model in parallel on top of Spark through Fugue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = pd.DataFrame({\"x_1\": [1, 1, 2, 2], \"x_2\":[1, 2, 2, 3]})\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "reg = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df: pd.DataFrame, model: LinearRegression) -> pd.DataFrame:\n",
    "    return df.assign(predicted=model.predict(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test data\n",
    "input_df = pd.DataFrame({\"x_1\": [3, 4, 6, 6], \"x_2\":[3, 3, 6, 6]})\n",
    "\n",
    "# test the predict function\n",
    "predict(input_df, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing to Spark\n",
    "\n",
    "We can bring the `predict()` function to Spark by using the Fugue `transform()` function and passing an engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import transform\n",
    "\n",
    "# create a spark dataframe\n",
    "sdf = spark.createDataFrame(input_df)\n",
    "\n",
    "# use Fugue transform to switch exection to spark\n",
    "result = transform(\n",
    "    df=sdf,\n",
    "    using=predict,\n",
    "    schema=\"*,predicted:double\",\n",
    "    params=dict(model=reg),\n",
    "    engine=spark\n",
    ")\n",
    "\n",
    "# display results\n",
    "print(type(result))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "This example will clarify what the `transform()` function is doing and how it is applied per partition of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"col1\": [\"a\",\"a\",\"a\",\"b\",\"b\",\"b\"], \n",
    "                   \"col2\": [1,2,3,4,5,6]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "\n",
    "def min_max(df:pd.DataFrame) -> List[Dict[str,Any]]:\n",
    "    return [{\"group\": df.iloc[0][\"col1\"], \n",
    "             \"max\": df['col2'].max(), \n",
    "             \"min\": df['col2'].min()}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = transform(\n",
    "    df=df, \n",
    "    using=min_max, \n",
    "    schema=\"group:str, max:int, min:int\",\n",
    "    partition={\"by\": \"col1\"},\n",
    "    engine=spark\n",
    "    )\n",
    "res.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fcd6e71927f6b3e5f4fa4280b4e8e6a66aa8d4365bb61cf7ef4017620fc09b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
