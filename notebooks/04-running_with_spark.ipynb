{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on Spark (with Databricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read in the data\n",
    "INPUT_DIR = os.path.abspath('data')\n",
    "WORKING_DIR = os.path.abspath(\"data/working\")\n",
    "calendar = pd.read_csv(f'{INPUT_DIR}/calendar.csv')\n",
    "sales = pd.read_csv(f'{INPUT_DIR}/sales_train_evaluation.csv')\n",
    "sell_prices = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Data Footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Iterable\n",
    "from datetime import date\n",
    "import pickle\n",
    "\n",
    "def prices_to_series(df:pd.DataFrame) -> List[Dict[str,Any]]:\n",
    "    # Assert each date has a price entry\n",
    "    assert df.shape[0] == (df.date.iloc[-1]-df.date.iloc[0]).days + 1\n",
    "    return [dict(store_id=df.iloc[0][\"store_id\"],\n",
    "                 item_id=df.iloc[0]['item_id'],\n",
    "                 price_start=df.iloc[0]['date'], \n",
    "                 prices=df[\"sell_price\"].tolist())]\n",
    "\n",
    "\n",
    "df = pd.DataFrame([[\"store1\",\"item1\",date(2020,1,2),2.2], \n",
    "                   [\"store1\",\"item1\",date(2020,1,3),3.3],\n",
    "                   [\"store1\",\"item1\", date(2020,1,4),4.4]], \n",
    "                   columns=[\"store_id\", \"item_id\", \"date\",\"sell_price\"])\n",
    "print(prices_to_series(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = sell_prices.merge(calendar[[\"date\",\"wm_yr_wk\"]], how=\"inner\", on=\"wm_yr_wk\")\n",
    "joined['date'] = pd.to_datetime(joined['date'])\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import transform\n",
    "\n",
    "sell_prices = transform(joined, \n",
    "                prices_to_series, \n",
    "                schema=\"store_id:str,item_id:str,price_start:date,prices:[float]\",\n",
    "                partition={\"by\": [\"store_id\", \"item_id\"], \"presort\": \"date asc\"})\n",
    "sell_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema: unique_id:str,item_id:str,store_id:str,sales_start:date,sales:[float]\n",
    "def sales_to_series(df:Iterable[List[Any]], start) -> Iterable[List[Any]]:\n",
    "    for row in df:\n",
    "        yield row[:2] + [row[4]] + [start, row[6:]]\n",
    "\n",
    "sales = transform(sales, sales_to_series, params={\"start\": calendar['date'].min()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = sales.merge(sell_prices, on=[\"item_id\", \"store_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Logic for Each Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_series(df:List[Dict[str,Any]]) -> pd.DataFrame:\n",
    "    row = df[0]\n",
    "    dr1 = pd.date_range(row[\"sales_start\"],periods=len(row[\"sales\"]), freq=\"d\")\n",
    "    df = pd.DataFrame({\"quantity\":row[\"sales\"]},index = dr1)\n",
    "    dr2 = pd.date_range(row[\"price_start\"],periods=len(row[\"prices\"]), freq=\"d\")\n",
    "    df[\"price\"] = pd.Series(row[\"prices\"],index = dr2)\n",
    "    df=df.dropna().reset_index()\n",
    "    df.columns=[\"ds\", \"quantity\", \"price\"]\n",
    "    df['unique_id'] = row['unique_id'] \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = format_series(combined.iloc[0:1].to_dict(\"records\"))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Cross Validation\n",
    "\n",
    "For timeseries cross validations, we perform the modelling with a sliding window of test sets. This is so we don't predict past data points with future information.\n",
    "\n",
    "![img](https://miro.medium.com/max/1204/1*qvdnPF8ETV9mFdMT0Y_BBA.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import Naive, CrostonClassic, IMAPA, ADIDA, AutoARIMA\n",
    "\n",
    "def run_model_cv(df: pd.DataFrame):\n",
    "  sf = StatsForecast(df=df, \n",
    "      models=[CrostonClassic(),\n",
    "        IMAPA(),\n",
    "        AutoARIMA()\n",
    "    ], \n",
    "      freq=\"D\",\n",
    "      n_jobs=1)\n",
    "  \n",
    "  return sf.cross_validation(h=28, n_windows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = run_model_cv(test)\n",
    "test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def calculate_metrics(cv_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    models = []\n",
    "    metrics = []\n",
    "    for model in [\"CrostonClassic\", \"IMAPA\", \"AutoARIMA\"]:\n",
    "        models.append(model)\n",
    "        metrics.append(mean_absolute_error(cv_df['y'], cv_df[model]))\n",
    "    out = pd.DataFrame({\"models\": models, \"metric\": metrics})\n",
    "    out['unique_id'] = cv_df.index[0]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    timeseries = format_series(df.to_dict(\"records\"))\n",
    "    model_cv = run_model_cv(timeseries)\n",
    "    metrics = calculate_metrics(model_cv).reset_index(drop=True)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform(combined.iloc[0:2], \n",
    "          process, \n",
    "          schema=\"models:str,metric:float,unique_id:str\", \n",
    "          partition={\"by\": \"unique_id\"},)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on Spark Cluster\n",
    "\n",
    "You can either use `databricks-connect` to connect to a Spark cluster or you can run this on Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = transform(combined.iloc[0:50], \n",
    "                    process, \n",
    "                    schema=\"models:str,metric:float,unique_id:str\", \n",
    "                    engine=spark, \n",
    "                    partition={\"by\": \"unique_id\"}).toPandas()\n",
    "\n",
    "results.to_parquet(f'{WORKING_DIR}/model_search.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_parquet(f'{WORKING_DIR}/model_search.parquet')\n",
    "best_models = results.sort_values('metric', ascending=True).groupby(\"unique_id\").first()\n",
    "best_models['models'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fcd6e71927f6b3e5f4fa4280b4e8e6a66aa8d4365bb61cf7ef4017620fc09b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
