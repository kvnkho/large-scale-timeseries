{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing with Fugue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Data\n",
    "\n",
    "We'll take a quick look at the data given to us to understand the problem more. Most of the code snippets here are taken from [Rob Mulla's Starter Notebook](https://www.kaggle.com/code/robikscube/m5-forecasting-starter-data-exploration). We're not going to go to deep to understand everything. We're only interested in setting up an end-to-end modelling pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read in the data\n",
    "INPUT_DIR = os.path.abspath('data')\n",
    "WORKING_DIR = os.path.abspath(\"data/working\")\n",
    "training_data = pd.read_csv(f'{INPUT_DIR}/sales_train_evaluation.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calendar_data():\n",
    "    df = pd.read_csv(f'{INPUT_DIR}/calendar.csv')\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List, Any, Dict\n",
    "from fugue import transform\n",
    "from datetime import timedelta\n",
    "\n",
    "start = get_calendar_data()['date'].min()\n",
    "\n",
    "# schema: unique_id:str,item_id:str,store_id:str,ds:date,y:int\n",
    "def format_sales(df:Iterable[List[Any]], start) -> Iterable[List[Any]]:\n",
    "    for row in df:\n",
    "        counter = 0\n",
    "        for y in row[6:]:\n",
    "            # help with convergence\n",
    "            if y == 0:\n",
    "                y = y + 0.01\n",
    "            date = start + timedelta(counter-1)\n",
    "            yield row[:2] + [row[4]] + [date, y]\n",
    "            counter=counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform(training_data.iloc[0:1], format_sales, params={\"start\": start})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = transform(training_data[0:100], \n",
    "                format_sales, \n",
    "                params={\"start\": start}, \n",
    "                engine=\"dask\")\n",
    "ddf.compute().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exogenous Regressors\n",
    "\n",
    "We want to add price in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')\n",
    "sell_prices.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_calendar_data().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "sampled_sales = training_data.iloc[0:2]\n",
    "calendar = get_calendar_data()\n",
    "start = calendar['date'].min()\n",
    "\n",
    "def process_data(sample=True) -> FugueWorkflow:\n",
    "    dag = FugueWorkflow()\n",
    "    if sample:\n",
    "        sales = dag.df(sampled_sales)\n",
    "    else:\n",
    "        sales = dag.load(f'{INPUT_DIR}/sales_train_evaluation.csv', header=True)\n",
    "    prices = dag.load(f'{INPUT_DIR}/sell_prices.csv', header=True)\n",
    "    calendar = dag.load(f'{INPUT_DIR}/calendar.csv', header=True).rename({\"date\": \"ds\"}).alter_columns(\"ds:date\")\n",
    "    sales = sales.transform(format_sales, params={\"start\": start})\n",
    "    combined = sales.join(calendar[[\"ds\",\"wm_yr_wk\"]], how=\"left_outer\")\\\n",
    "                    .join(prices, how=\"inner\")\n",
    "    combined.show()\n",
    "    combined.save(f\"{WORKING_DIR}/combined.parquet\")\n",
    "    return dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = process_data(sample=True)\n",
    "dag.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run on the full dataset and get the full combined file, you can execute:\n",
    "\n",
    "```python\n",
    "dag = process_data(sample=False)\n",
    "dag.run(spark)\n",
    "```\n",
    "\n",
    "where Spark is the SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchichal Preprocessing\n",
    "\n",
    "We need to keep the hierchichal columns for aggregating later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = get_calendar_data()['date'].min()\n",
    "\n",
    "# schema: unique_id:str,item_id:str,dept_id:str,cat_id:str,store_id:str,state_id:str,ds:date,y:int\n",
    "def format_sales_hierarchical(df:Iterable[List[Any]], start) -> Iterable[List[Any]]:\n",
    "    for row in df:\n",
    "        counter = 0\n",
    "        for y in row[6:]:\n",
    "            # help with convergence\n",
    "            if y == 0:\n",
    "                y = y + 0.01\n",
    "            date = start + timedelta(counter-1)\n",
    "            yield row[:6] + [date, y]\n",
    "            counter=counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform(training_data, format_sales_hierarchical, params={\"start\": start}, engine=\"spark\", save_path=f\"{WORKING_DIR}/hierarchical.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fcd6e71927f6b3e5f4fa4280b4e8e6a66aa8d4365bb61cf7ef4017620fc09b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
